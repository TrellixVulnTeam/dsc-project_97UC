{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "raw_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Connect to google drive to access requirements.txt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "yCGBRFmB0vZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade --force-reinstall `cat /content/gdrive/My\\ Drive/requirements.txt`"
      ],
      "metadata": {
        "id": "OPj02rhC2tq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load required dependencies \n",
        "\n",
        "import torch\n",
        "import zipfile\n",
        "import torchaudio\n",
        "from glob import glob\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer"
      ],
      "metadata": {
        "id": "uHhWWrFpATXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that cuda is available\n",
        "\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "_Vc9XrzHz7VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeechToText:\n",
        "  \"\"\"\n",
        "  Convert audio files to text using the Silero speech-to-text model\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, text_files):\n",
        "    self.text_files = text_files\n",
        "\n",
        "  def convert(self):\n",
        "    # Load Silero model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')  \n",
        "\n",
        "    silero_model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models',\n",
        "                                          model='silero_stt',\n",
        "                                          language='en', \n",
        "                                          device=device)  \n",
        "    (read_batch, split_into_batches,\n",
        "    read_audio, prepare_model_input) = utils  \n",
        "\n",
        "    # Use batch normalization to improve model performance\n",
        "    batches = split_into_batches(enhanced_audio, batch_size=10)\n",
        "    input = prepare_model_input(read_batch(batches[0]),\n",
        "                            device=device)\n",
        "\n",
        "    output = silero_model(input)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def format_text(self, output):\n",
        "    # Add punctuation and recapitilize characters in output from speech to text conversion\n",
        "    silero_model, example_texts, languages, punct, apply_te = torch.hub.load(repo_or_dir='snakers4/silero-models',\n",
        "                                                                      model='silero_te')\n",
        "    output_text = ''\n",
        "    \n",
        "    for example in output:\n",
        "      print(decoder(example.cpu()));\n",
        "      input_text = decoder(example.cpu())\n",
        "      print(apply_te(input_text, lan='en'))\n",
        "      output_text += apply_te(input_text, lan='en')\n",
        "    \n",
        "    print(output_text)\n",
        "\n",
        "    return output_text\n",
        "\n",
        "if __name__ == \"main\": \n",
        "\n",
        "    test_files = glob('ted_talk.wav')\n",
        "\n",
        "    speech_to_text = SpeechToText(text_files)\n",
        "    output = ruspeech_to_textn.convert() \n",
        "    output_text = speech_to_text.format_text()"
      ],
      "metadata": {
        "id": "X5CxmUi5Bifd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AbstractiveSummarization:\n",
        "  \"\"\"\n",
        "  Perform abstractive text summarization using the Pegasus model \n",
        "  \"\"\"\n",
        "  def __init__(self, input_text):\n",
        "    self.input_text = input_text\n",
        "\n",
        "  def Summarize(self):    \n",
        "    # Load Pegasus model\n",
        "    model_name = \"google/pegasus-xsum\"\n",
        "    pegasus_model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "    tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "    tokens = tokenizer(self.input_text, batched=True, truncation=True, padding='longest', return_tensors='pt').to(device)\n",
        "    \n",
        "    # Generate new set of token sequences\n",
        "    summary = pegasus_model.generate(**tokens)\n",
        "    decoded_summary = tokenizer.decode(summary[0])\n",
        "\n",
        "    return decoded_summary\n",
        "\n",
        "if __name__ == \"main\": \n",
        "\n",
        "  summarize = AbstractiveSummarization(output_text)"
      ],
      "metadata": {
        "id": "gPC93MlyYPXn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
